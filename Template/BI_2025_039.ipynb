{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582888a-d357-42c6-8ec6-a5a7647519d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "#!pip install -r requirements.txt\n",
    "#!pip install \"numpy==1.26.4\" \"pandas==2.1.4\"\n",
    "#!pip install plotly\n",
    "#!pip uninstall -y starvers\n",
    "#!pip install plotly\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_11906257'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '39'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12402153'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_11906257'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    'qudt-unit': 'http://qudt.org/vocab/unit/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Poj\" .',\n",
    "f':{student_a} foaf:familyName \"Netsiri\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"12402153\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"01234567\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Michael\" .',\n",
    "f':{student_b} foaf:familyName \"Wolkerstorfer\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"11906257\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"76543210\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore_data_path = \"data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## 1. Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee41a0-d332-4edc-ba4e-d4f39c933440",
   "metadata": {},
   "source": [
    "**a. Define and describe the data source and a scenario in which a business analytics task based on the data set you identified should be solved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae011c-52c7-44f8-b843-d8d03a76fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "Dataset: Superstore Dataset\n",
    "Source: https://www.kaggle.com/datasets/vivek468/superstore-dataset-final\n",
    "\n",
    "The dataset contains order-level retail information:\n",
    "- Order/Ship Dates\n",
    "- Customer, Segment, Region\n",
    "- Product Category, Sub-Category\n",
    "- Sales (prediction target)\n",
    "- Quantity, Discount\n",
    "- Profit (used only for interpretation)\n",
    "\n",
    "Scenario:\n",
    "A retail company aims to improve order-level sales forecasting. Better sales prediction supports\n",
    "pricing strategy, promotion planning, and more accurate demand and inventory management.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262c599-ffb7-4eab-b5ef-38bfe31f3a39",
   "metadata": {},
   "source": [
    "**b. Clearly define and describe the Business Objectives.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8381c3-70bd-4ec5-932a-c30028597b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "Primary Business Objective:\n",
    "Predict sales at the order level to strengthen revenue planning and operational decision-making.\n",
    "\n",
    "Secondary Objectives:\n",
    "- Identify key drivers of sales (e.g., category, discount, region).\n",
    "- Improve pricing and promotion decisions.\n",
    "- Support inventory allocation and demand planning.\n",
    "- Highlight high-performing regions and product lines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfacf2e-c23c-4b85-bec2-09de21ccb51f",
   "metadata": {},
   "source": [
    "**c. Clearly define and describe the Business Success Criteria.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1ec7c-ba32-4fa9-b14e-3327810b317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "Business Success Criteria:\n",
    "- Improved forecasting accuracy that reduces stockouts and overstocking.\n",
    "- More informed pricing and discount decisions.\n",
    "- Fewer low-value or unprofitable orders.\n",
    "- Adoption of predictions in operational workflows.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebe013-68dd-4d20-b6f5-3067382f1db6",
   "metadata": {},
   "source": [
    "**d. Clearly define and describe the Data Mining Goals.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9faea-080e-4423-8e4d-bad8c7dc6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "Data Mining (ML) Goals:\n",
    "- Build a regression model to predict Sales based on order characteristics.\n",
    "- Engineer interpretable features (date fields, category aggregates).\n",
    "- Avoid data leakage by excluding Profit and profit-derived attributes.\n",
    "- Achieve stable generalization performance.\n",
    "- Identify meaningful factors influencing sales.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df5a79-847c-4b7c-9dee-fb2ecb15924a",
   "metadata": {},
   "source": [
    "**e. Clearly define and describe the Data Mining Success Criteria.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f160d-6fbf-4123-a2c1-b02523d62607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "Data Mining Success Criteria:\n",
    "- Achieve acceptable predictive performance (reasonable RÂ² and RMSE).\n",
    "- No overfitting (similar training and test results).\n",
    "- Stable feature importance rankings.\n",
    "- No missing-value issues or target leakage.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f70b3-7e54-4ac5-804e-b62a448ed7d6",
   "metadata": {},
   "source": [
    "**f. Are there any AI risk aspects that may require specific consideration?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27087b28-197e-47cb-ab4c-344e85125a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "AI Risk Aspects:\n",
    "1. Data Leakage:\n",
    "   Profit or post-outcome information must not appear in the features.\n",
    "\n",
    "2. Bias:\n",
    "   Historical patterns may bias results toward specific regions or customer groups.\n",
    "\n",
    "3. Over-Reliance:\n",
    "   Sales depend on external factors not captured in the dataset (seasonality, supply issues).\n",
    "\n",
    "4. Model Misuse:\n",
    "   Automated discounting or pricing actions may lead to unfair treatment.\n",
    "\n",
    "5. Explainability:\n",
    "   Transparent reasoning is necessary for stakeholder trust; use interpretable features\n",
    "   and feature importance methods.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "bu_ass_uuid_executor = \"bb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\" # Generate once\n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .', # Connect Activity to Parent Business Understanding Phase Activity\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# 1a. Define and describe the data source and a scenario.\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b. Clearly define and describe the Business Objectives.\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c. Clearly define and describe the Business Success Criteria.\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d. Clearly define and describe the Data Mining Goals.\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e. Clearly define and describe the Data Mining Success Criteria.\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f. Are there any AI risk aspects that may require specific consideration?\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "data_understanding_phase_executor = [\n",
    "f':data_understanding_phase rdf:type prov:Activity .',\n",
    "f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "engine.insert(data_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore_data_path = \"data\"\n",
    "\n",
    "load_superstore_data_code_writer = student_a\n",
    "def load_superstore_data()-> pd.DataFrame:\n",
    "\n",
    "    ### Load data\n",
    "    input_file = os.path.join(superstore_data_path, 'Superstore.csv')\n",
    "    raw_data  = pd.read_csv(input_file, encoding=\"latin1\", sep=',', header = 0)\n",
    "\n",
    "    raw_data [\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], errors=\"coerce\")\n",
    "    raw_data [\"Ship Date\"] = pd.to_datetime(raw_data[\"Ship Date\"], errors=\"coerce\")\n",
    "\n",
    "    loaded_data = raw_data\n",
    "    return loaded_data \n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_superstore_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"b8bac193-c4e6-4e31-9134-b23e001e279c\" # Generate once\n",
    "load_superstore_data_executor = [\n",
    "    f':load_superstore_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(load_superstore_data_executor, prefixes=prefixes)\n",
    "\n",
    "ld_ass_uuid_writer = \"c600e15c-87a9-4e2a-be85-b6c2a3014210\" # Generate once\n",
    "ld_report = \"\"\"\n",
    "Load all superstore data and convert order date and ship date to datetime. \n",
    "\"\"\"\n",
    "load_superstore_data_activity = [\n",
    "    ':load_superstore_data rdf:type prov:Activity .',\n",
    "    ':load_superstore_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_superstore_data rdfs:comment \\'Load superstore data\\' .',\n",
    "    f':load_superstore_data rdfs:comment \"\"\"{ld_report}\"\"\" .', \n",
    "    f':load_superstore_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_superstore_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_superstore_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_superstore_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # INPUT of activity\n",
    "    ':load_superstore_data prov:used :superstore_csv .',\n",
    "    ':superstore_csv rdf:type prov:Entity .',\n",
    "    ':superstore_csv prov:wasDerivedFrom :superstore_data_path .',\n",
    "    ':superstore_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :superstore_csv .',\n",
    "    # OUTPUT of activity\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_superstore_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "engine.insert(load_superstore_data_activity, prefixes=prefixes)\n",
    "\n",
    "# Further descibe the raw data using Croissant\n",
    "\n",
    "raw_data_triples = [\n",
    "    # Dataset-level description\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \"Superstore Sales Dataset\" .',\n",
    "    ':raw_data sc:description \"Order-level retail sales transactions from a superstore.\" .',\n",
    "    # Distribution: the CSV file\n",
    "    ':superstore_csv rdf:type cr:FileObject .',\n",
    "    ':superstore_csv sc:name \"Superstore.csv\" .',\n",
    "    ':superstore_csv sc:encodingFormat \"text/csv\" .',\n",
    "    ':raw_data sc:distribution :superstore_csv .',\n",
    "    # RecordSet describing the tabular structure\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \"Superstore order table\" .',\n",
    "    ':raw_recordset sc:description \"Table of order-level retail transactions with customer, product, and sales information.\" .',\n",
    "    ':raw_recordset cr:source :superstore_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "    # Row ID (integer)\n",
    "    ':raw_recordset cr:field :field_row_id .',\n",
    "    ':field_row_id rdf:type cr:Field .',\n",
    "    ':field_row_id sc:name \"Row ID\" .',\n",
    "    ':field_row_id sc:description \"Unique row identifier for each record.\" .',\n",
    "    ':field_row_id cr:dataType xsd:integer .',\n",
    "    # Order ID (string)\n",
    "    ':raw_recordset cr:field :field_order_id .',\n",
    "    ':field_order_id rdf:type cr:Field .',\n",
    "    ':field_order_id sc:name \"Order ID\" .',\n",
    "    ':field_order_id sc:description \"Identifier of the customer order.\" .',\n",
    "    ':field_order_id cr:dataType xsd:string .',\n",
    "    # Order Date (date)\n",
    "    ':raw_recordset cr:field :field_order_date .',\n",
    "    ':field_order_date rdf:type cr:Field .',\n",
    "    ':field_order_date sc:name \"Order Date\" .',\n",
    "    ':field_order_date sc:description \"Date when the order was placed.\" .',\n",
    "    ':field_order_date cr:dataType xsd:date .',\n",
    "    # Ship Date (date)\n",
    "    ':raw_recordset cr:field :field_ship_date .',\n",
    "    ':field_ship_date rdf:type cr:Field .',\n",
    "    ':field_ship_date sc:name \"Ship Date\" .',\n",
    "    ':field_ship_date sc:description \"Date when the order was shipped.\" .',\n",
    "    ':field_ship_date cr:dataType xsd:date .',\n",
    "    # Ship Mode (string)\n",
    "    ':raw_recordset cr:field :field_ship_mode .',\n",
    "    ':field_ship_mode rdf:type cr:Field .',\n",
    "    ':field_ship_mode sc:name \"Ship Mode\" .',\n",
    "    ':field_ship_mode sc:description \"Shipping service level (e.g., Second Class, Standard Class).\" .',\n",
    "    ':field_ship_mode cr:dataType xsd:string .',\n",
    "    # Customer ID (string)\n",
    "    ':raw_recordset cr:field :field_customer_id .',\n",
    "    ':field_customer_id rdf:type cr:Field .',\n",
    "    ':field_customer_id sc:name \"Customer ID\" .',\n",
    "    ':field_customer_id sc:description \"Identifier of the customer placing the order.\" .',\n",
    "    ':field_customer_id cr:dataType xsd:string .',\n",
    "    # Customer Name (string)\n",
    "    ':raw_recordset cr:field :field_customer_name .',\n",
    "    ':field_customer_name rdf:type cr:Field .',\n",
    "    ':field_customer_name sc:name \"Customer Name\" .',\n",
    "    ':field_customer_name sc:description \"Full name of the customer.\" .',\n",
    "    ':field_customer_name cr:dataType xsd:string .',\n",
    "    # Segment (string)\n",
    "    ':raw_recordset cr:field :field_segment .',\n",
    "    ':field_segment rdf:type cr:Field .',\n",
    "    ':field_segment sc:name \"Segment\" .',\n",
    "    ':field_segment sc:description \"Customer segment (Consumer, Corporate, Home Office).\" .',\n",
    "    ':field_segment cr:dataType xsd:string .',\n",
    "    # Country (string)\n",
    "    ':raw_recordset cr:field :field_country .',\n",
    "    ':field_country rdf:type cr:Field .',\n",
    "    ':field_country sc:name \"Country\" .',\n",
    "    ':field_country sc:description \"Country where the order was placed.\" .',\n",
    "    ':field_country cr:dataType xsd:string .',\n",
    "    # City (string)\n",
    "    ':raw_recordset cr:field :field_city .',\n",
    "    ':field_city rdf:type cr:Field .',\n",
    "    ':field_city sc:name \"City\" .',\n",
    "    ':field_city sc:description \"City of the shipping address.\" .',\n",
    "    ':field_city cr:dataType xsd:string .',\n",
    "    # State (string)\n",
    "    ':raw_recordset cr:field :field_state .',\n",
    "    ':field_state rdf:type cr:Field .',\n",
    "    ':field_state sc:name \"State\" .',\n",
    "    ':field_state sc:description \"State of the shipping address.\" .',\n",
    "    ':field_state cr:dataType xsd:string .',\n",
    "    # Zip Code (string)\n",
    "    ':raw_recordset cr:field :field_postal_code .',\n",
    "    ':field_postal_code rdf:type cr:Field .',\n",
    "    ':field_postal_code sc:name \"Postal Code\" .',\n",
    "    ':field_postal_code sc:description \"ZIP code of the shipping address.\" .',\n",
    "    ':field_postal_code cr:dataType xsd:string .',\n",
    "    # Region (string)\n",
    "    ':raw_recordset cr:field :field_region .',\n",
    "    ':field_region rdf:type cr:Field .',\n",
    "    ':field_region sc:name \"Region\" .',\n",
    "    ':field_region sc:description \"Geographical sales region.\" .',\n",
    "    ':field_region cr:dataType xsd:string .',\n",
    "    # Product ID (string)\n",
    "    ':raw_recordset cr:field :field_product_id .',\n",
    "    ':field_product_id rdf:type cr:Field .',\n",
    "    ':field_product_id sc:name \"Product ID\" .',\n",
    "    ':field_product_id sc:description \"Identifier of the purchased product.\" .',\n",
    "    ':field_product_id cr:dataType xsd:string .',\n",
    "    # Category (string)\n",
    "    ':raw_recordset cr:field :field_category .',\n",
    "    ':field_category rdf:type cr:Field .',\n",
    "    ':field_category sc:name \"Category\" .',\n",
    "    ':field_category sc:description \"Product category.\" .',\n",
    "    ':field_category cr:dataType xsd:string .',\n",
    "    # Sub-Category (string)\n",
    "    ':raw_recordset cr:field :field_sub_category .',\n",
    "    ':field_sub_category rdf:type cr:Field .',\n",
    "    ':field_sub_category sc:name \"Sub-Category\" .',\n",
    "    ':field_sub_category sc:description \"Product sub-category.\" .',\n",
    "    ':field_sub_category cr:dataType xsd:string .',\n",
    "    # Product Name (string)\n",
    "    ':raw_recordset cr:field :field_product_name .',\n",
    "    ':field_product_name rdf:type cr:Field .',\n",
    "    ':field_product_name sc:name \"Product Name\" .',\n",
    "    ':field_product_name sc:description \"Name of the product.\" .',\n",
    "    ':field_product_name cr:dataType xsd:string .',\n",
    "    # Sales (double)\n",
    "    ':raw_recordset cr:field :field_sales .',\n",
    "    ':field_sales rdf:type cr:Field .',\n",
    "    ':field_sales sc:name \"Sales\" .',\n",
    "    ':field_sales sc:description \"Total sales amount per item in the order.\" .',\n",
    "    ':field_sales cr:dataType xsd:double .',\n",
    "    # Quantity (integer)\n",
    "    ':raw_recordset cr:field :field_quantity .',\n",
    "    ':field_quantity rdf:type cr:Field .',\n",
    "    ':field_quantity sc:name \"Quantity\" .',\n",
    "    ':field_quantity sc:description \"Number of units sold.\" .',\n",
    "    ':field_quantity cr:dataType xsd:integer .',\n",
    "    # Discount (double)\n",
    "    ':raw_recordset cr:field :field_discount .',\n",
    "    ':field_discount rdf:type cr:Field .',\n",
    "    ':field_discount sc:name \"Discount\" .',\n",
    "    ':field_discount sc:description \"Discount fraction applied to the item.\" .',\n",
    "    ':field_discount cr:dataType xsd:double .',\n",
    "    # Profit (double)\n",
    "    ':raw_recordset cr:field :field_profit .',\n",
    "    ':field_profit rdf:type cr:Field .',\n",
    "    ':field_profit sc:name \"Profit\" .',\n",
    "    ':field_profit sc:description \"Profit for the item (Sales minus cost).\" .',\n",
    "    ':field_profit cr:dataType xsd:double .',\n",
    "]\n",
    "\n",
    "engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "# The output of the load activity is a dataset that can be described with Croissant\n",
    "data_triples = [\n",
    "    # Loaded dataset\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "    ':data sc:name \"Loaded Superstore Sales Dataset\" .',\n",
    "    ':data sc:description \"Order-level retail dataset after loading Superstore.csv and parsing order and ship dates.\" .',\n",
    "    # RecordSet for the in-memory table\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "    ':recordset sc:name \"Loaded Superstore order table\" .',\n",
    "    ':recordset sc:description \"In-memory table of order-level retail transactions with parsed date fields and derived features.\" .',\n",
    "    ':data cr:recordSet :recordset .',\n",
    "    # Reuse fields defined for :raw_data by attaching them to :recordset\n",
    "    ':recordset cr:field :field_row_id .',\n",
    "    ':recordset cr:field :field_order_id .',\n",
    "    ':recordset cr:field :field_order_date .',\n",
    "    ':recordset cr:field :field_ship_date .',\n",
    "    ':recordset cr:field :field_ship_mode .',\n",
    "    ':recordset cr:field :field_customer_id .',\n",
    "    ':recordset cr:field :field_customer_name .',\n",
    "    ':recordset cr:field :field_segment .',\n",
    "    ':recordset cr:field :field_country .',\n",
    "    ':recordset cr:field :field_city .',\n",
    "    ':recordset cr:field :field_state .',\n",
    "    ':recordset cr:field :field_postal_code .',\n",
    "    ':recordset cr:field :field_region .',\n",
    "    ':recordset cr:field :field_product_id .',\n",
    "    ':recordset cr:field :field_category .',\n",
    "    ':recordset cr:field :field_sub_category .',\n",
    "    ':recordset cr:field :field_product_name .',\n",
    "    ':recordset cr:field :field_sales .',\n",
    "    ':recordset cr:field :field_quantity .',\n",
    "    ':recordset cr:field :field_discount .',\n",
    "    ':recordset cr:field :field_profit .',\n",
    "    # Additional derived field in the loaded data\n",
    "    ':recordset cr:field :field_day_of_week .',\n",
    "    ':field_day_of_week rdf:type cr:Field .',\n",
    "    ':field_day_of_week sc:name \"day_of_week\" .',\n",
    "    ':field_day_of_week sc:description \"Day of the week derived from the order date (e.g., Monday, Tuesday).\" .',\n",
    "    ':field_day_of_week cr:dataType xsd:string .',\n",
    "]\n",
    "\n",
    "engine.insert(data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also add the units to the fields\n",
    "units_triples = [\n",
    "    # Quantity is count of items (\"each\")\n",
    "    ':field_quantity qudt:unit qudt-unit:EA .',\n",
    "    # Sales and Profit are U.S. dollars\n",
    "    ':field_sales qudt:unit qudt-unit:USD .',\n",
    "    ':field_profit qudt:unit qudt-unit:USD .',\n",
    "]\n",
    "\n",
    "engine.insert(units_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96180552-3cbd-455d-8616-44f398f5c2e0",
   "metadata": {},
   "source": [
    "**a. Attribute types, units of measurement, and the semantics of attributes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c768f-63f5-4725-9685-34349de35cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_description_code_writer = student_a\n",
    "\n",
    "def map_dtype_to_xsd(dtype) -> str:\n",
    "    if str(dtype).startswith(\"int\"):\n",
    "        return \"xsd:integer\"\n",
    "    if str(dtype).startswith(\"float\"):\n",
    "        return \"xsd:double\"\n",
    "    if \"datetime\" in str(dtype):\n",
    "        return \"xsd:dateTime\"\n",
    "    return \"xsd:string\"\n",
    "\n",
    "\n",
    "def describe_attributes(data: pd.DataFrame) -> dict:\n",
    "    results = {}\n",
    "    \n",
    "    unit_map = {\n",
    "        \"Sales\": \"qudt-unit:USD\",\n",
    "        \"Profit\": \"qudt-unit:USD\",\n",
    "        \"Quantity\": \"qudt-unit:EA\",\n",
    "    }\n",
    "\n",
    "    for col in data.columns:\n",
    "        dtype = data[col].dtype\n",
    "        xsd_type = map_dtype_to_xsd(dtype)\n",
    "\n",
    "        if \"datetime\" in str(dtype):\n",
    "            semantic_type = \"date/time\"\n",
    "        elif str(dtype).startswith((\"int\", \"float\")):\n",
    "            semantic_type = \"numeric\"\n",
    "        else:\n",
    "            semantic_type = \"categorical / text\"\n",
    "\n",
    "        col_info = {\n",
    "            \"pandas_dtype\": str(dtype),\n",
    "            \"xsd_datatype\": xsd_type,\n",
    "            \"semantic_type\": semantic_type,\n",
    "        }\n",
    "\n",
    "        if col in unit_map:\n",
    "            col_info[\"unit\"] = unit_map[col]\n",
    "        else:\n",
    "            col_info[\"unit\"] = None\n",
    "\n",
    "        results[col] = col_info\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "start_time_attr = now()\n",
    "attribute_description_report = describe_attributes(data)\n",
    "end_time_attr = now()\n",
    "\n",
    "print(attribute_description_report)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# 1. Activity: automatically describing attributes\n",
    "attr_ass_uuid_executor = \"1d1f3a1b-6f7a-4f4c-b5d6-0aaefc5a1234\"  # use a unique UUID\n",
    "describe_attributes_executor = [\n",
    "    f':describe_attributes prov:qualifiedAssociation :{attr_ass_uuid_executor} .',\n",
    "    f':{attr_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{attr_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{attr_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(describe_attributes_executor, prefixes=prefixes)\n",
    "\n",
    "attr_ass_uuid_writer = \"5a3dcb3f-ec9b-4d64-9e23-0d65b8a81234\"  # use a unique UUID\n",
    "attr_comment = \"\"\"\n",
    "Automatically analyzing attribute types and units of measurement for the loaded\n",
    "Superstore dataset.\n",
    "\"\"\"\n",
    "\n",
    "describe_attributes_activity = [\n",
    "    ':describe_attributes rdf:type prov:Activity .',\n",
    "    ':describe_attributes sc:isPartOf :data_understanding_phase .',\n",
    "    ':describe_attributes rdfs:comment \"Data Understanding\" .',\n",
    "    f':describe_attributes rdfs:comment \"\"\"{attr_comment}\"\"\" .',\n",
    "    f':describe_attributes prov:startedAtTime \"{start_time_attr}\"^^xsd:dateTime .',\n",
    "    f':describe_attributes prov:endedAtTime \"{end_time_attr}\"^^xsd:dateTime .',\n",
    "    f':describe_attributes prov:qualifiedAssociation :{attr_ass_uuid_writer} .',\n",
    "    f':{attr_ass_uuid_writer} prov:agent :{attribute_description_code_writer} .',\n",
    "    f':{attr_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{attr_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':describe_attributes prov:used :data .',\n",
    "    # Report entity\n",
    "    ':attribute_description_report rdf:type prov:Entity .',\n",
    "    f':attribute_description_report rdfs:comment \"\"\"{str(attribute_description_report)}\"\"\" .',\n",
    "    ':attribute_description_report prov:wasGeneratedBy :describe_attributes .',\n",
    "]\n",
    "engine.insert(describe_attributes_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94dfb7-328c-432b-b7e7-1f66f03eabca",
   "metadata": {},
   "source": [
    "**b. Statistical properties describing the dataset including correlations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b4793-5fad-4c9a-89dd-abd662f916b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_properties_code_writer = student_a\n",
    "\n",
    "def analyze_statistical_properties(data: pd.DataFrame, columns=('Sales', 'Quantity', 'Discount', 'Profit')) -> dict:\n",
    "    results = {}\n",
    "    tmp = data.copy().reset_index(drop=True)\n",
    "\n",
    "    per_column_stats = {}\n",
    "    existing_cols = []\n",
    "    missing_cols = []\n",
    "\n",
    "    for col in columns:\n",
    "        if col not in tmp.columns:\n",
    "            missing_cols.append(col)\n",
    "            per_column_stats[col] = {\"error\": \"Column not found\"}\n",
    "            continue\n",
    "\n",
    "        existing_cols.append(col)\n",
    "        values = tmp[col].astype(float)\n",
    "\n",
    "        stats = {\n",
    "            \"count\": int(values.count()),\n",
    "            \"mean\": float(values.mean()),\n",
    "            \"median\": float(values.median()),\n",
    "            \"std\": float(values.std()),\n",
    "            \"min\": float(values.min()),\n",
    "            \"max\": float(values.max()),\n",
    "            \"skewness\": float(values.skew())\n",
    "        }\n",
    "\n",
    "        per_column_stats[col] = stats\n",
    "\n",
    "    results[\"per_column_stats\"] = per_column_stats\n",
    "    results[\"columns_used\"] = existing_cols\n",
    "    results[\"columns_missing\"] = missing_cols\n",
    "\n",
    "    numeric_df = tmp[existing_cols].astype(float)\n",
    "    corr_matrix = numeric_df.corr(method='pearson')\n",
    "    results[\"correlation_matrix\"] = corr_matrix.to_dict()\n",
    "  \n",
    "    return results\n",
    "\n",
    "\n",
    "start_time_stats = now()\n",
    "statistical_properties_report = analyze_statistical_properties(data)\n",
    "end_time_stats = now()\n",
    "\n",
    "print(statistical_properties_report)\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "stats_ass_uuid_executor = \"9f4a9d83-0f2a-4f63-8d9f-1ad4c9b8e301\"  # unique UUID\n",
    "analyze_statistical_properties_executor = [\n",
    "    f':analyze_statistical_properties prov:qualifiedAssociation :{stats_ass_uuid_executor} .',\n",
    "    f':{stats_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{stats_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{stats_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(analyze_statistical_properties_executor, prefixes=prefixes)\n",
    "\n",
    "stats_ass_uuid_writer = \"3b1e7bc2-6712-4b12-9c56-2e4be9577f0a\"  # unique UUID\n",
    "stats_comment = \"\"\"\n",
    "Analyzing statistical properties and correlations of Sales, Quantity, Discount, and Profit \n",
    "in the Superstore dataset. Descriptive statistics show that Sales and Profit are highly \n",
    "right-skewed, with a small number of very large transactions creating long heavy tails, \n",
    "while Quantity and Discount exhibit moderate skewness. These distribution characteristics \n",
    "highlight the presence of rare but influential extreme values.\n",
    "A Pearson correlation matrix indicates a moderate positive relationship between Sales \n",
    "and Profit, weak associations involving Quantity, and a mild negative correlation \n",
    "between Discount and Profit. \n",
    "\"\"\"\n",
    "\n",
    "analyze_statistical_properties_activity = [\n",
    "    ':analyze_statistical_properties rdf:type prov:Activity .',\n",
    "    ':analyze_statistical_properties sc:isPartOf :data_understanding_phase .',\n",
    "    ':analyze_statistical_properties rdfs:comment \"Data Understanding\" .',\n",
    "    f':analyze_statistical_properties rdfs:comment \"\"\"{stats_comment}\"\"\" .',\n",
    "    f':analyze_statistical_properties prov:startedAtTime \"{start_time_stats}\"^^xsd:dateTime .',\n",
    "    f':analyze_statistical_properties prov:endedAtTime \"{end_time_stats}\"^^xsd:dateTime .',\n",
    "    f':analyze_statistical_properties prov:qualifiedAssociation :{stats_ass_uuid_writer} .',\n",
    "    f':{stats_ass_uuid_writer} prov:agent :{statistical_properties_code_writer} .',\n",
    "    f':{stats_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{stats_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # Input of activity:\n",
    "    ':analyze_statistical_properties prov:used :data .',\n",
    "    # Output of activity: \n",
    "    ':statistical_properties_report rdf:type prov:Entity .',\n",
    "    f':statistical_properties_report rdfs:comment \"\"\"{str(statistical_properties_report)}\"\"\" .',\n",
    "    ':statistical_properties_report prov:wasGeneratedBy :analyze_statistical_properties .',\n",
    "]\n",
    "engine.insert(analyze_statistical_properties_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bab9ff-0f58-4543-a0c4-5efa3234811b",
   "metadata": {},
   "source": [
    "**c. Data quality aspects, e.g. missing values and their potential effects and reasons, uneven distributions in certain attribute types, plausibility of values, outliers, information available on data provenance and data cleansing applied before, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0580e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_code_writer = student_a\n",
    "\n",
    "def analyze_data_quality(\n",
    "    data: pd.DataFrame,\n",
    "    outlier_threshold: float = 2.2,\n",
    "    outlier_columns=('Sales', 'Quantity', 'Discount', 'Profit'),\n",
    "    completeness_columns=None,\n",
    ") -> dict:\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    tmp = data.copy().reset_index(drop=True)\n",
    "    n_rows = len(tmp)\n",
    "\n",
    "    # COMPLETENESS CHECK\n",
    "    if completeness_columns is None:\n",
    "        completeness_columns = list(tmp.columns)\n",
    "\n",
    "    column_stats = {}\n",
    "\n",
    "    for col in completeness_columns:\n",
    "        missing_count = int(tmp[col].isna().sum())\n",
    "        non_missing_count = n_rows - missing_count\n",
    "        missing_pct = (missing_count / n_rows * 100.0) if n_rows > 0 else 0.0\n",
    "\n",
    "        column_stats[col] = {\n",
    "            \"total_rows\": n_rows,\n",
    "            \"missing_count\": missing_count,\n",
    "            \"non_missing_count\": non_missing_count,\n",
    "            \"missing_pct\": missing_pct,\n",
    "        }\n",
    "\n",
    "    rows_with_any_missing = int(tmp.isna().any(axis=1).sum())\n",
    "    rows_with_no_missing = n_rows - rows_with_any_missing\n",
    "\n",
    "    if tmp.shape[1] > 0:\n",
    "        rows_gt_50pct_missing = int(\n",
    "            (tmp.isna().sum(axis=1) / tmp.shape[1] > 0.5).sum()\n",
    "        )\n",
    "    else:\n",
    "        rows_gt_50pct_missing = 0\n",
    "\n",
    "    row_stats = {\n",
    "        \"total_rows\": n_rows,\n",
    "        \"rows_with_any_missing\": rows_with_any_missing,\n",
    "        \"rows_with_no_missing\": rows_with_no_missing,\n",
    "        \"rows_with_any_missing_pct\": (\n",
    "            rows_with_any_missing / n_rows * 100.0 if n_rows > 0 else 0.0\n",
    "        ),\n",
    "        \"rows_gt_50pct_missing\": rows_gt_50pct_missing,\n",
    "    }\n",
    "\n",
    "    results[\"completeness\"] = {\n",
    "        \"column_completeness\": column_stats,\n",
    "        \"row_completeness\": row_stats,\n",
    "    }\n",
    "\n",
    "    # DUPLICATE CHECK\n",
    "    full_duplicates = int(tmp.duplicated().sum())\n",
    "    key_duplicates = int(tmp.duplicated(subset=[\"Order ID\", \"Product ID\"]).sum())\n",
    "\n",
    "    duplicate_rows = tmp[tmp.duplicated(subset=[\"Order ID\", \"Product ID\"], keep=False)]\n",
    "\n",
    "    results[\"duplicates\"] = {\n",
    "        \"full_duplicate_rows\": full_duplicates,\n",
    "        \"duplicate_key_rows\": key_duplicates,\n",
    "        \"duplicate_key_records_preview\": duplicate_rows.head(10).to_dict()\n",
    "    }\n",
    "\n",
    "  \n",
    "    outlier_results = {}\n",
    "\n",
    "    for col in outlier_columns:\n",
    "        values = tmp[col].astype(float)\n",
    "        mean = values.mean()\n",
    "        std = values.std()\n",
    "\n",
    "        if std == 0 or np.isnan(std):\n",
    "            outlier_results[col] = []\n",
    "            continue\n",
    "\n",
    "        z_scores = (values - mean) / std\n",
    "        mask = np.abs(z_scores) > outlier_threshold\n",
    "        outlier_indices = values[mask].index\n",
    "\n",
    "        outlier_info = [\n",
    "            {\"index\": int(idx), \"z_score\": float(z_scores.loc[idx])}\n",
    "            for idx in outlier_indices\n",
    "        ]\n",
    "\n",
    "        outlier_results[col] = outlier_info\n",
    "\n",
    "    results[\"outliers\"] = outlier_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "start_time_dq = now()\n",
    "data_quality_report = analyze_data_quality(data)\n",
    "end_time_dq = now()\n",
    "\n",
    "dq = data_quality_report\n",
    "\n",
    "\n",
    "print(\"Missing Values (%):\")\n",
    "for col, stats in dq[\"completeness\"][\"column_completeness\"].items():\n",
    "    print(f\"{col}: {stats['missing_pct']:.2f}%\")\n",
    "\n",
    "print(\"\\nDuplicate Summary:\")\n",
    "print(f\"Full duplicated rows: {dq['duplicates']['full_duplicate_rows']}\")\n",
    "print(f\"Duplicate (Order ID, Product ID): {dq['duplicates']['duplicate_key_rows']}\")\n",
    "\n",
    "print(\"\\nOutliers per Column:\")\n",
    "for col, items in dq[\"outliers\"].items():\n",
    "    print(f\"{col}: {len(items)}\")\n",
    "\n",
    "row_comp = dq[\"completeness\"][\"row_completeness\"]\n",
    "dup_info = dq[\"duplicates\"]\n",
    "\n",
    "outlier_summary = \", \".join(\n",
    "    f\"{col}: {len(items)}\" for col, items in dq[\"outliers\"].items()\n",
    ")\n",
    "\n",
    "dq_summary = (\n",
    "    \"Data Quality Summary - \"\n",
    "    f\"total rows: {row_comp['total_rows']}, \"\n",
    "    f\"rows with any missing values: {row_comp['rows_with_any_missing']}, \"\n",
    "    f\"full duplicated rows: {dup_info['full_duplicate_rows']}, \"\n",
    "    f\"duplicate (Order ID, Product ID) rows: {dup_info['duplicate_key_rows']}, \"\n",
    "    f\"outliers per column: {outlier_summary}.\"\n",
    ")\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "dq_ass_uuid_executor = \"0f7b4f50-8d3f-4a1e-9de0-3c2c7cf0a111\"\n",
    "analyze_data_quality_executor = [\n",
    "    f':analyze_data_quality prov:qualifiedAssociation :{dq_ass_uuid_executor} .',\n",
    "    f':{dq_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dq_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dq_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(analyze_data_quality_executor, prefixes=prefixes)\n",
    "\n",
    "dq_ass_uuid_writer = \"e9b6a7e4-2e5e-4f66-bd32-9b1cf5b7e222\"\n",
    "\n",
    "dq_comment = \"\"\"\n",
    "Analyzing data quality aspects of the Superstore dataset including:\n",
    "\n",
    "1. Completeness Analysis:\n",
    "   All attributes exhibit 0% missing values at both column and row levels. This indicates a \n",
    "   fully complete dataset, reducing the risk of bias introduced by imputation or selective \n",
    "   data omission. The absence of missing values suggests consistent data collection \n",
    "   procedures or prior data cleansing applied by the dataset provider.\n",
    "\n",
    "2. Duplicate Detection:\n",
    "   No fully duplicated rows were found. However, several business-key duplicates were \n",
    "   detected for the combination (Order ID, Product ID). These duplicates show identical \n",
    "   descriptive attributes (customer, product, region, ship mode, etc.) but differ in \n",
    "   Row ID, Sales, Quantity, and Profit. This pattern suggests that an order line may \n",
    "   have been modified or updated after initial entryâe.g., quantity adjustments, corrections, \n",
    "   or customer-initiated changes. If not addressed, these duplicates may cause \n",
    "   double-counting, distort descriptive statistics, inflate sales totals, and bias \n",
    "   model training.\n",
    "\n",
    "3. Statistical Properties and Distribution Characteristics:\n",
    "   The Sales attribute shows extremely strong right-skewness (skew = 12.97), with a median \n",
    "   of 54.49 and a maximum of 22,638.48, indicating that most transactions are small while \n",
    "   a few very large orders create a long heavy tail. Profit is similarly skewed (skew = 7.56) \n",
    "   and ranges from â6,599.98 to 8,399.98, reflecting both large gains and substantial losses. \n",
    "   Quantity (skew = 1.28) and Discount (skew = 1.68) are moderately skewed but much more \n",
    "   constrained in range. Correlation analysis reveals expected patterns: Sales correlates \n",
    "   moderately with Profit (r = 0.48), while Quantity has only weak relationships with other \n",
    "   variables. Discount shows a small negative correlation with Profit (r = â0.22), consistent \n",
    "   with higher discounts reducing profitability. These distributional characteristics and \n",
    "   weak-to-moderate correlations indicate that the dataset is dominated by typical low-value \n",
    "   orders punctuated by rare but influential extreme transactions, which must be handled \n",
    "   carefully during modeling.\n",
    "\n",
    "4. Outlier Detection:\n",
    "   Outlier analysis using z-scores and scatter plots shows several extreme values in Sales, \n",
    "   Profit, Quantity, and Discount. The plots reveal high-value and low-profit transactions, \n",
    "   unusually large quantities, and atypical discount levels. These outliers may stem from \n",
    "   bulk orders, special promotions, adjustments, or rare business scenarios. Although some \n",
    "   may be valid, they distort statistical properties, correlations, and model behavior, \n",
    "   requiring appropriate treatment (e.g., capping or transformation) during data preparation.\n",
    "\"\"\"\n",
    "\n",
    "analyze_data_quality_activity = [\n",
    "    ':analyze_data_quality rdf:type prov:Activity .',\n",
    "    ':analyze_data_quality sc:isPartOf :data_understanding_phase .',\n",
    "    f':analyze_data_quality rdfs:comment \"\"\"{dq_comment}\"\"\" .',\n",
    "    f':analyze_data_quality prov:startedAtTime \"{start_time_dq}\"^^xsd:dateTime .',\n",
    "    f':analyze_data_quality prov:endedAtTime \"{end_time_dq}\"^^xsd:dateTime .',\n",
    "    f':analyze_data_quality prov:qualifiedAssociation :{dq_ass_uuid_writer} .',\n",
    "    f':{dq_ass_uuid_writer} prov:agent :{data_quality_code_writer} .',\n",
    "    f':{dq_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dq_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':analyze_data_quality prov:used :data .',\n",
    "    ':data_quality_report rdf:type prov:Entity .',\n",
    "    f':data_quality_report rdfs:comment \"\"\"{dq_summary}\"\"\" .',\n",
    "    ':data_quality_report prov:wasGeneratedBy :analyze_data_quality .',\n",
    "]\n",
    "engine.insert(analyze_data_quality_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30e34e-6e9d-4c2e-803a-23b780cee581",
   "metadata": {},
   "source": [
    "**d. Visual exploration of data properties and hypotheses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781e016-c770-43d2-871a-f4f4ab7378b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_exploration_code_writer = student_a\n",
    "\n",
    "def plot_outlier_scatter(\n",
    "    data: pd.DataFrame,\n",
    "    outlier_report: dict,\n",
    "    columns=('Sales', 'Quantity', 'Discount', 'Profit')\n",
    "):\n",
    "    tmp = data.copy().reset_index(drop=True)\n",
    "    num_cols = [c for c in columns if c in tmp.columns]\n",
    "\n",
    "    outlier_indices = {\n",
    "        col: set(int(e[\"index\"]) for e in entries if isinstance(entries, list))\n",
    "        for col, entries in outlier_report.items()\n",
    "    }\n",
    "\n",
    "    for i in range(len(num_cols)):\n",
    "        for j in range(i + 1, len(num_cols)):\n",
    "            x_col = num_cols[i]\n",
    "            y_col = num_cols[j]\n",
    "\n",
    "            plt.figure(figsize=(5, 3))\n",
    "            plt.scatter(\n",
    "                tmp[x_col],\n",
    "                tmp[y_col],\n",
    "                alpha=0.4,\n",
    "                s=8,                   # <<< smaller dots\n",
    "                label=\"All points\"\n",
    "            )\n",
    "\n",
    "            # Mark outliers in either column\n",
    "            idx_union = outlier_indices.get(x_col, set()) | outlier_indices.get(y_col, set())\n",
    "            if idx_union:\n",
    "                mask = tmp.index.isin(idx_union)\n",
    "                plt.scatter(\n",
    "                    tmp.loc[mask, x_col],\n",
    "                    tmp.loc[mask, y_col],\n",
    "                    edgecolor=\"red\",\n",
    "                    facecolor=\"none\",\n",
    "                    s=30,\n",
    "                    label=\"Outliers\"\n",
    "                )\n",
    "\n",
    "            plt.xlabel(x_col)\n",
    "            plt.ylabel(y_col)\n",
    "            plt.title(f\"Scatter Plot with Outliers: {x_col} vs {y_col}\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def plot_histograms(\n",
    "    data: pd.DataFrame,\n",
    "    columns=('Sales', 'Quantity', 'Discount', 'Profit'),\n",
    "    bins=50\n",
    "):\n",
    "    tmp = data.copy().reset_index(drop=True)\n",
    "    existing_cols = [col for col in columns if col in tmp.columns]\n",
    "\n",
    "    for col in existing_cols:\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.hist(tmp[col].astype(float), bins=bins, alpha=0.7)\n",
    "        plt.title(f\"Histogram of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.4)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(data: pd.DataFrame, columns=('Sales', 'Quantity', 'Discount', 'Profit')):\n",
    "    tmp = data.copy().reset_index(drop=True)\n",
    "    existing_cols = [c for c in columns if c in tmp.columns]\n",
    "\n",
    "    numeric_df = tmp[existing_cols].astype(float)\n",
    "    corr_matrix = numeric_df.corr(method='pearson')\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        xticklabels=existing_cols,\n",
    "        yticklabels=existing_cols,\n",
    "        square=True\n",
    "    )\n",
    "    plt.title(\"Correlation Matrix Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "vis_columns = ('Sales', 'Quantity', 'Discount', 'Profit')\n",
    "start_time_vis = now()\n",
    "\n",
    "plot_outlier_scatter(data, dq[\"outliers\"], columns=vis_columns) #Use dq[\"outliers\"] from data quality section\n",
    "plot_histograms(data, columns=vis_columns, bins=50)\n",
    "corr_matrix = plot_correlation_matrix(data, columns=vis_columns)\n",
    "\n",
    "end_time_vis = now()\n",
    "\n",
    "print(\"Correlation matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "vis_ass_uuid_executor = \"c2f9a6f4-60ab-4b93-9f39-4e2cfcc5a901\"\n",
    "\n",
    "visual_exploration_executor = [\n",
    "    f':visual_exploration prov:qualifiedAssociation :{vis_ass_uuid_executor} .',\n",
    "    f':{vis_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{vis_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{vis_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "\n",
    "engine.insert(visual_exploration_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "vis_ass_uuid_writer = \"6e11f2a3-0e9a-4a65-b0c0-9c434e8b2a77\"\n",
    "\n",
    "vis_comment = \"\"\"\n",
    "Visual exploration of Sales, Quantity, Discount, and Profit including:\n",
    "- Scatter plots highlighting outliers detected earlier\n",
    "- Histograms illustrating distribution shapes\n",
    "- Correlation matrix heatmap visualizing linear relationships\n",
    "\n",
    "These visualizations support hypothesis generation and provide insight into\n",
    "distribution skewness, outlier behavior, and attribute interactions as\n",
    "required in the Data Understanding phase.\n",
    "\"\"\"\n",
    "\n",
    "visual_exploration_activity = [\n",
    "    ':visual_exploration rdf:type prov:Activity .',\n",
    "    ':visual_exploration sc:isPartOf :data_understanding_phase .',\n",
    "\n",
    "    # human-readable description\n",
    "    f':visual_exploration rdfs:comment \"\"\"{vis_comment}\"\"\" .',\n",
    "\n",
    "    # timing\n",
    "    f':visual_exploration prov:startedAtTime \"{start_time_vis}\"^^xsd:dateTime .',\n",
    "    f':visual_exploration prov:endedAtTime \"{end_time_vis}\"^^xsd:dateTime .',\n",
    "\n",
    "    # writer association (provenance)\n",
    "    f':visual_exploration prov:qualifiedAssociation :{vis_ass_uuid_writer} .',\n",
    "    f':{vis_ass_uuid_writer} prov:agent :{visual_exploration_code_writer} .',\n",
    "    f':{vis_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{vis_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # inputs used for visualization\n",
    "    ':visual_exploration prov:used :data .',\n",
    "    ':visual_exploration prov:used :data_quality_report .',\n",
    "\n",
    "    # output entity\n",
    "    ':visual_exploration_report rdf:type prov:Entity .',\n",
    "    ':visual_exploration_report sc:isPartOf :data_understanding_phase .',\n",
    "    ':visual_exploration_report prov:wasGeneratedBy :visual_exploration .',\n",
    "    f':visual_exploration_report rdfs:comment \"\"\"Visual exploration figures were generated and interpreted.\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(visual_exploration_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46f948-9ffc-49da-a0b5-f31f41a56f6a",
   "metadata": {},
   "source": [
    "**e. Evaluate and document whether the data set contains attributes that are potentially ethically sensitive, minority classes or underrepresented data groups, unbalanced distributions with respect to bias (to guide over- and under-sampling, micro- and macro evaluation criteria).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed1c7a-afcd-449c-9a39-2ac2da32850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "du_ethics_comment = \"\"\"\n",
    "The Superstore dataset does not contain direct sensitive personal information \n",
    "such as race, gender, age, income, or health data. However, several attributes \n",
    "can indirectly reflect demographic or socioeconomic structures:\n",
    "\n",
    "- 'Region', 'State', and 'City' may reflect geographic disparities in access to \n",
    "  goods and logistics performance.\n",
    "- 'Segment' may represent customer types with inherently different spending \n",
    "  power (Home Office, Corporate, Consumer), which implies potential \n",
    "  representation imbalance among customer groups.\n",
    "- Some regions appear less represented in the data, implying potential \n",
    "  underrepresentation of certain geographical markets.\n",
    "- Uneven distributions in order frequency across customer segments may create \n",
    "  implicit model bias if predictions consistently favor the most represented group.\n",
    "\"\"\"\n",
    "\n",
    "du_ethics = [\n",
    "    ':du_ethics rdf:type prov:Entity .',\n",
    "    ':du_ethics sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_ethics rdfs:comment \"\"\"{du_ethics_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(du_ethics, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fcb36-db3f-4496-a98a-5e7816842dca",
   "metadata": {},
   "source": [
    "**f. What potential risks and additional types of bias exist in the data? What questions would you need to have answered by an external expert in order to determine potential bias or data quality issues?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3faac-807a-4902-bec8-70f26e4fc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "du_bias_risks_comment = \"\"\"\n",
    "Potential risks and additional bias sources in the dataset include:\n",
    "\n",
    "1. Geographic Bias:\n",
    "   Certain states and regions appear significantly over- or underrepresented.\n",
    "   This imbalance could distort model behavior, making predictions overly tuned \n",
    "   to high-frequency regions.\n",
    "\n",
    "2. Customer Segment Bias:\n",
    "   Customer segments are not evenly distributed, which may bias predictions \n",
    "   toward the majority class (e.g., Consumer segment).\n",
    "\n",
    "3. Pricing and Discount Bias:\n",
    "   Discount values vary between product categories. A model may incorrectly \n",
    "   treat discount-heavy categories as inherently lower-value, introducing \n",
    "   operational bias.\n",
    "\n",
    "4. Profit and Sales Distribution Skew:\n",
    "   Strong right skewness in 'Sales' and 'Profit' may cause models to overweight \n",
    "   high-value transactions while under-representing low-volume customers.\n",
    "\n",
    "Questions requiring expert clarification:\n",
    "- Are regional sales volumes reflective of real market size, or are they \n",
    "  sampling artifacts?\n",
    "- Are discounts systematically applied to specific categories or customer groups?\n",
    "- Should low-profit or highly discounted transactions be treated differently \n",
    "  in forecasting and evaluation?\n",
    "- Are there external economic, seasonal, or promotional factors not represented \n",
    "  in this dataset that systematically affect certain groups?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "du_bias_risks = [\n",
    "    ':du_bias_risks rdf:type prov:Entity .',\n",
    "    ':du_bias_risks sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_bias_risks rdfs:comment \"\"\"{du_bias_risks_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(du_bias_risks, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311308c-2646-4ee1-ab4f-be272006837f",
   "metadata": {},
   "source": [
    "**g. Which actions are likely required in data preparation based on this analysis?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b4638-8277-4cf1-8418-74ed415fb4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "du_prep_actions_comment = \"\"\"\n",
    "Based on the findings from the Data Understanding phase, the following data \n",
    "preparation actions are recommended:\n",
    "\n",
    "1. Outlier Handling:\n",
    "   Extreme values in Sales, Profit, and Discount can distort model behavior and \n",
    "   disproportionately influence error metrics. These will be handled through \n",
    "   removal or capping depending on severity. In addition, Sales will undergo a \n",
    "   log transformation to reduce heteroscedasticity, compress extreme values, \n",
    "   and improve model stability.\n",
    "\n",
    "2. Missing Value Treatment:\n",
    "   The completeness analysis confirmed that the dataset contains 0% missing values \n",
    "   across all attributes (identifiers, dates, geographic descriptors, product \n",
    "   fields, and numeric measures). Because no missingness is present, no imputation \n",
    "   strategies are required.\n",
    "\n",
    "3. Duplicate Treatment:\n",
    "   Duplicate records were detected based on the business key (Order ID, Product ID). \n",
    "   In all duplicate pairs, every attribute except Row ID, Sales, Quantity, and \n",
    "   Profit is identical. This pattern strongly suggests that the customer modified \n",
    "   an order line while retaining all categorical and descriptive fields. To avoid \n",
    "   double counting and to preserve only the most recent version of each order, \n",
    "   the first occurrence of each duplicate pair will be removed, and the later \n",
    "   (updated) record will be retained.\n",
    "\n",
    "4. Feature Engineering:\n",
    "   Derive informative temporal features (e.g., day of week, month, year) from the \n",
    "   Order Date to capture seasonal or cyclical purchasing patterns.\n",
    "\n",
    "5. Skewness Consideration:\n",
    "   The Sales distribution is highly right-skewed, with most transactions near \n",
    "   zero and a long tail of high-value orders. Such skewness can degrade regression \n",
    "   performance by over-emphasizing rare extreme cases. A log transformation will \n",
    "   therefore be applied to Sales. Other skewed attributes (e.g., Profit) will not \n",
    "   be transformed, as the downstream CatBoost model is robust to non-normal \n",
    "   feature distributions and does not rely on scale-sensitive assumptions.\n",
    "\n",
    "6. Categorical Encoding:\n",
    "   Segment, Region, Category, and Sub-Category will be encoded using approaches \n",
    "   compatible with the selected modeling method (e.g., CatBoost-native categorical \n",
    "   encoding or one-hot encoding if needed). This ensures that categorical \n",
    "   attributes are appropriately represented in the model without inflating \n",
    "   dimensionality unnecessarily.\n",
    "\n",
    "7. Plausibility and Consistency Checks:\n",
    "   Additional integrity checks will ensure that:\n",
    "   - Discount values fall within the expected [0, 1] range,\n",
    "   - Postal codes follow valid formats,\n",
    "   - Sales and Quantity values are strictly positive,\n",
    "   - Ship Date is chronologically after Order Date.\n",
    "\n",
    "8. Representation and Bias Considerations:\n",
    "   Several geographic and customer segments appear underrepresented. If these \n",
    "   imbalances are found to affect model fairness or accuracy, resampling, \n",
    "   reweighting, or stratified evaluation may be applied to ensure that model \n",
    "   performance is not biased toward majority groups.\n",
    "\n",
    "Overall, these preparation steps ensure that the dataset is clean, consistent, \n",
    "representative, and suitable for robust and fair predictive modeling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "du_prep_actions = [\n",
    "    ':du_prep_actions rdf:type prov:Entity .',\n",
    "    ':du_prep_actions sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_prep_actions rdfs:comment \"\"\"{du_prep_actions_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(du_prep_actions, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "### 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "f':data_preparation_phase rdf:type prov:Activity .',\n",
    "f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .', \n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465762f0-cb43-40f2-b85f-5df74226ea58",
   "metadata": {},
   "source": [
    "**Handling all outliers that were identified in the Data Understanding phase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d076f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_outliers_code_writer = student_b\n",
    "def handle_outliers(df:pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle outliers according to the following strategy:\n",
    "    \n",
    "    Sales:    strongly right-skewed, strictly positive, heavy-tailed\n",
    "              -> log-transform\n",
    "    \n",
    "    Quantity: skewed, discrete integers, usually small, with rare extreme values\n",
    "              bulk orders no errors, but can heavily influence regression\n",
    "              dropping removes important customer behavior, log-transform distorts the semantics\n",
    "              -> cap extreme values but preserve all entries\n",
    "    \n",
    "    Discount: high values reflect real promotions, not errors\n",
    "              naturally capped at 1\n",
    "              -> keep entries\n",
    "    \n",
    "    Profit:   highly skewed, valid extreme gains/losses\n",
    "              negative values prevent log-transform, but extremes distort modeling\n",
    "              -> cap extreme values but preserve all entries\n",
    "    \"\"\"\n",
    "    df_no_outlier = df.copy()\n",
    "    \n",
    "    # Extract the collected outlier indices per column\n",
    "    outlier_indices = {\n",
    "        col: [entry[\"index\"] for entry in entries]\n",
    "        for col, entries in outliers_report.items()\n",
    "    }\n",
    "    \n",
    "    # Sales\n",
    "    if not (df[\"Sales\"] > 0).all():\n",
    "        raise ValueError(\"Invalid data detected: some Sales values are not positive in the Data Preparation phase.\")\n",
    "    df_no_outlier[\"Sales_log\"] = np.log(df_no_outlier[\"Sales\"])\n",
    "    \n",
    "    # Quantity\n",
    "    if \"Quantity\" in outlier_indices:\n",
    "        q_high = df_no_outlier[\"Quantity\"].quantile(0.99)\n",
    "        rows = outlier_indices[\"Quantity\"]\n",
    "        df_no_outlier.loc[rows, \"Quantity\"] = df_no_outlier.loc[rows, \"Quantity\"].clip(upper=q_high)\n",
    "    \n",
    "    # Profit\n",
    "    if \"Profit\" in outlier_indices:\n",
    "        p_low = df_no_outlier[\"Profit\"].quantile(0.005)\n",
    "        p_high = df_no_outlier[\"Profit\"].quantile(0.995)\n",
    "        rows = outlier_indices[\"Profit\"]\n",
    "        df_no_outlier.loc[rows, \"Profit\"] = df_no_outlier.loc[rows, \"Profit\"].clip(lower=p_low, upper=p_high)\n",
    "\n",
    "    return df_no_outlier\n",
    "\n",
    "start_time_td = now()\n",
    "no_outlier_data = handle_outliers(data, dq[\"outliers\"])\n",
    "end_time_td = now()\n",
    "\n",
    "display(no_outlier_data)\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "ro_ass_uuid_executor = \"ec7e81e1-86ea-475a-a8d4-c7d8ee535488\"\n",
    "handle_outliers_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "td_ass_uuid_writer = \"1405f15a-3545-4014-a962-637f3c10a137\"\n",
    "td_comment = \"\"\"\n",
    "Handling all outliers that were identified in the Data Understanding phase.\n",
    "The following approach was used for each column represented in the outlier summary:\n",
    "- Sales: strongly right-skewed, strictly positive, heavy-tailed -> apply log-transform (new column: Sales_log)\n",
    "- Quantity: discrete, skewed with rare bulk orders -> cap only detected outlier rows using the 99th percentile as upper bound (preserve all entries)\n",
    "- Discount: high values reflect valid promotions and are naturally capped at 1 -> keep all entries unchanged\n",
    "- Profit: highly skewed with valid extreme gains/losses -> cap only detected outlier rows using the 0.5th percentile as lower bound and the 99.5th percentile as upper bound\n",
    "\"\"\"\n",
    "handle_outliers_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_outliers rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_outliers prov:used :data .',\n",
    "    ':handle_outliers prov:used :data_qaulity_report .',\n",
    "    ':no_outlier_data rdf:type prov:Entity .',\n",
    "    ':no_outlier_data prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':no_outlier_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(handle_outliers_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100cff7-8fd5-4ba1-8913-b4f1ccdfda35",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8800ce26b8f3e2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Missing Value Treatment in the Data Preparation phase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20b8e8-7d7f-4df5-ba38-62704f020c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_missing_values_code_writer = student_b\n",
    "def handle_missing_values(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Confirm that the dataset contains no missing values after outlier handling.\n",
    "    This step implements the Data Preparation decision:\n",
    "    - In the Data Understanding phase no missing values were observed\n",
    "    - Therefore, no imputation, row/column deletion, or other modifications are applied\n",
    "    - This activity verifies that there are still no missing values after previous\n",
    "      preparation steps\n",
    "    \"\"\"\n",
    "    mv_counts = df.isna().sum()\n",
    "    if not (mv_counts == 0).all():\n",
    "        raise ValueError(\"Unexpected missing values found in Data Preparation phase.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "start_time_mv = now()\n",
    "no_missing_data = handle_missing_values(no_outlier_data)\n",
    "end_time_mv = now()\n",
    "\n",
    "display(no_missing_data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "mv_ass_uuid_executor = \"4239ba2c-d9c6-4e74-970f-fa60ddb63c2b\"\n",
    "handle_missing_values_executor = [\n",
    "    f':handle_missing_values prov:qualifiedAssociation :{mv_ass_uuid_executor} .',\n",
    "    f':{mv_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mv_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mv_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_missing_values_executor, prefixes=prefixes)\n",
    "\n",
    "mv_ass_uuid_writer = \"b97d9261-421c-447a-a7d6-50a90f4f8714\"\n",
    "mv_comment = \"\"\"\n",
    "Missing Value Treatment in the Data Preparation phase.\n",
    "In the Data Understanding phase no missing values were observed.\n",
    "Therefore, no imputation, row/column deletion, or other modifications are applied.\n",
    "This activity verifies that there are still no missing values after previous preparation steps.\n",
    "\"\"\"\n",
    "\n",
    "handle_missing_values_activity = [\n",
    "    ':handle_missing_values rdf:type prov:Activity .',\n",
    "    ':handle_missing_values sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_missing_values rdfs:comment \\'Data Preparation\\' .',\n",
    "    f':handle_missing_values rdfs:comment \"\"\"{mv_comment}\"\"\" .',\n",
    "    f':handle_missing_values prov:startedAtTime \"{start_time_mv}\"^^xsd:dateTime .',\n",
    "    f':handle_missing_values prov:endedAtTime \"{end_time_mv}\"^^xsd:dateTime .',\n",
    "    f':handle_missing_values prov:qualifiedAssociation :{mv_ass_uuid_writer} .',\n",
    "    f':{mv_ass_uuid_writer} prov:agent :{handle_missing_values_code_writer} .',\n",
    "    f':{mv_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mv_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_missing_values prov:used :no_outlier_data .',\n",
    "    ':no_missing_data rdf:type prov:Entity .',\n",
    "    ':no_missing_data prov:wasGeneratedBy :handle_missing_values .',\n",
    "    ':no_missing_data prov:wasDerivedFrom :no_outlier_data .',\n",
    "]\n",
    "engine.insert(handle_missing_values_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1be96-8ce8-46e3-8ce2-3acebd58d1fc",
   "metadata": {},
   "source": [
    "**Duplicate Treatment in the Data Preparation phase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e5056-f0ab-4703-864c-07a0b2ffd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_treatment_code_writer = student_b\n",
    "def handle_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle duplicate records in the dataset according to the following logic:\n",
    "\n",
    "    - Duplicates are defined by identical (Order ID, Product ID) pairs.\n",
    "    - In the Data Understanding phase we assumed that this is because orders were edited or updated\n",
    "    - The assumption is that the later record represents the corrected or final version of the order\n",
    "    - Therefore, earlier occurrences are removed and only the most recent occurrence is retained\n",
    "\n",
    "    No new duplicates can exist because previous steps in the Data preparation phase did not modify the key attributes \"Order ID\" and \"Product ID\".\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Keep the last (updated) version of each duplicate order line\n",
    "    df_no_duplicate = df_copy.drop_duplicates(\n",
    "        subset=[\"Order ID\", \"Product ID\"],\n",
    "        keep=\"last\"\n",
    "    )\n",
    "    return df_no_duplicate\n",
    "\n",
    "\n",
    "start_time_dup = now()\n",
    "no_duplicate_data = handle_duplicates(no_missing_data)\n",
    "end_time_dup = now()\n",
    "\n",
    "display(no_duplicate_data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "dup_ass_uuid_executor = \"e34b95af-3cf8-4ba6-ad43-bfe17ceed1d8\"\n",
    "handle_duplicates_executor = [\n",
    "    f':handle_duplicates prov:qualifiedAssociation :{dup_ass_uuid_executor} .',\n",
    "    f':{dup_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dup_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dup_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_duplicates_executor, prefixes=prefixes)\n",
    "\n",
    "dup_ass_uuid_writer = \"d8694288-7fd7-42af-85f4-a5035747d350\"\n",
    "\n",
    "dup_comment = \"\"\"\n",
    "Duplicate Treatment in the Data Preparation phase.\n",
    "Duplicates are defined by identical (Order ID, Product ID) pairs.\n",
    "In the Data Understanding phase we assumed that this is because orders were edited or updated.\n",
    "The assumption is that the later record represents the corrected or final version of the order.\n",
    "Therefore, earlier occurrences are removed and only the most recent occurrence is retained.\n",
    "No new duplicates can exist because previous steps in the Data preparation phase did not modify the key attributes \"Order ID\" and \"Product ID\".\n",
    "\"\"\"\n",
    "\n",
    "handle_duplicates_activity = [\n",
    "    ':handle_duplicates rdf:type prov:Activity .',\n",
    "    ':handle_duplicates sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_duplicates rdfs:comment \\'Data Preparation\\' .',\n",
    "    f':handle_duplicates rdfs:comment \"\"\"{dup_comment}\"\"\" .',\n",
    "    f':handle_duplicates prov:startedAtTime \"{start_time_dup}\"^^xsd:dateTime .',\n",
    "    f':handle_duplicates prov:endedAtTime \"{end_time_dup}\"^^xsd:dateTime .',\n",
    "    f':handle_duplicates prov:qualifiedAssociation :{dup_ass_uuid_writer} .',\n",
    "    f':{dup_ass_uuid_writer} prov:agent :{duplicate_treatment_code_writer} .',\n",
    "    f':{dup_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dup_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_duplicates prov:used :no_missing_data .',\n",
    "    ':no_duplicate_data rdf:type prov:Entity .',\n",
    "    ':no_duplicate_data prov:wasGeneratedBy :handle_duplicates .',\n",
    "    ':no_duplicate_data prov:wasDerivedFrom :no_missing_data .',\n",
    "]\n",
    "engine.insert(handle_duplicates_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f78f4-6971-4e3a-97a4-52ffbc2f5a54",
   "metadata": {},
   "source": [
    "**Analyze options and potential for derived attributes: Feature Engineering.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49edccc-6bfc-4205-bd9e-b62cdea6b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_code_writer = student_b\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Derive temporal features from Order Date:\n",
    "    - order_year\n",
    "    - order_month\n",
    "    - order_weekofyear\n",
    "    - order_dayofweek\n",
    "    - order_dayofmonth\n",
    "\n",
    "    These features describe when the order was placed and help represent seasonal patterns, monthly trends and weekly cycles.\n",
    "    \"\"\"\n",
    "    df_feat = df.copy()\n",
    "\n",
    "    df_feat[\"order_year\"]       = df_feat[\"Order Date\"].dt.year\n",
    "    df_feat[\"order_month\"]      = df_feat[\"Order Date\"].dt.month\n",
    "    df_feat[\"order_weekofyear\"] = df_feat[\"Order Date\"].dt.isocalendar().week.astype(int)\n",
    "    df_feat[\"order_dayofweek\"]  = df_feat[\"Order Date\"].dt.dayofweek\n",
    "    df_feat[\"order_dayofmonth\"] = df_feat[\"Order Date\"].dt.day\n",
    "\n",
    "    return df_feat\n",
    "\n",
    "start_time_fe = now()\n",
    "engineered_data = engineer_features(no_duplicate_data)\n",
    "end_time_fe = now()\n",
    "\n",
    "display(engineered_data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "fe_ass_uuid_executor = \"c3f19b23-aa7f-4425-ad36-67d2b5d2b35b\"\n",
    "feature_engineering_executor = [\n",
    "    f':engineer_features prov:qualifiedAssociation :{fe_ass_uuid_executor} .',\n",
    "    f':{fe_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{fe_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{fe_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(feature_engineering_executor, prefixes=prefixes)\n",
    "\n",
    "fe_ass_uuid_writer = \"332756c3-cf4e-4357-b2cf-8fe0b75e779b\"\n",
    "fe_comment = \"\"\"\n",
    "Analyze options and potential for derived attributes: Feature Engineering.\n",
    "We derive temporal features from the Order Date: order_year, order_month, order_weekofyear, order_dayofweek, order_dayofmonth.\n",
    "These features describe when the order was placed and help represent seasonal patterns, monthly trends and weekly cycles.\n",
    "They help the ML model capture cyclic and seasonal variation.\n",
    "We do not introduce data leakage since the Order Date is already known at the time of the sale.\n",
    "We also considered engineering interaction features, such as combining Quantity with Discount or Category with Region.\n",
    "Since CatBoost automatically models such interactions for categorical and numerical variables, we did not expect a high additional benefit from adding them manually.\n",
    "\"\"\"\n",
    "\n",
    "feature_engineering_activity = [\n",
    "    ':engineer_features rdf:type prov:Activity .',\n",
    "    ':engineer_features sc:isPartOf :data_preparation_phase .',\n",
    "    ':engineer_features rdfs:comment \"Data Preparation\" .',\n",
    "    f':engineer_features rdfs:comment \"\"\"{fe_comment}\"\"\" .',\n",
    "    f':engineer_features prov:startedAtTime \"{start_time_fe}\"^^xsd:dateTime .',\n",
    "    f':engineer_features prov:endedAtTime \"{end_time_fe}\"^^xsd:dateTime .',\n",
    "    f':engineer_features prov:qualifiedAssociation :{fe_ass_uuid_writer} .',\n",
    "    f':{fe_ass_uuid_writer} prov:agent :{feature_engineering_code_writer} .',\n",
    "    f':{fe_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{fe_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':engineer_features prov:used :no_duplicate_data .',\n",
    "    ':engineered_data rdf:type prov:Entity .',\n",
    "    ':engineered_data prov:wasGeneratedBy :engineer_features .',\n",
    "    ':engineered_data prov:wasDerivedFrom :no_duplicate_data .',\n",
    "]\n",
    "\n",
    "engine.insert(feature_engineering_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371ce33-8292-4963-aca7-ba44eddf2ec3",
   "metadata": {},
   "source": [
    "**Categorical Encoding Verification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05010dee-0031-4218-a099-eb8ada9b4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_encoding_code_writer = student_b\n",
    "def verify_categorical_types(df: pd.DataFrame, categorical_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Verify that the four main categorical modeling features are stored as\n",
    "    strings/object dtype so CatBoost can apply its internal ordered target encoding.\n",
    "    According to the CatBoost documentation, it can automatically handle categorical features\n",
    "    and does not require one-hot encoding. Categorical values should be provided as strings or pandas 'object' dtype.\n",
    "    Since CatBoost performs its own encoding, no manual encoding is applied in the Data Preparation phase.\n",
    "    \"\"\"\n",
    "    for col in categorical_cols:\n",
    "        if df[col].dtype not in [\"object\", \"category\"]:\n",
    "            raise TypeError(\n",
    "                \"Column '\" + col + \"' must be of dtype object/category for CatBoost automatic categorical encoding.\"\n",
    "            )\n",
    "    return df\n",
    "\n",
    "# primary categorical modeling features from Data Understanding phase\n",
    "categorical_features = [\n",
    "    \"Segment\",\n",
    "    \"Region\",\n",
    "    \"Category\",\n",
    "    \"Sub-Category\"\n",
    "]\n",
    "\n",
    "start_time_ce = now()\n",
    "categorical_data = verify_categorical_types(engineered_data, categorical_features)\n",
    "end_time_ce = now()\n",
    "\n",
    "display(categorical_data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "ce_ass_uuid_executor = \"5ad74ad6-16f8-483a-bcfa-2ea061e9c5f1\"\n",
    "categorical_encoding_executor = [\n",
    "    f':verify_categorical_types prov:qualifiedAssociation :{ce_ass_uuid_executor} .',\n",
    "    f':{ce_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ce_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ce_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(categorical_encoding_executor, prefixes=prefixes)\n",
    "\n",
    "ce_ass_uuid_writer = \"04a5a8b5-b020-4034-b2e2-7dd50d358426\"\n",
    "ce_comment = \"\"\"\n",
    "Categorical Encoding Verification.\n",
    "We verify that the four main categorical modeling features are stored as strings/object dtype so CatBoost can apply its internal ordered target encoding.\n",
    "According to the CatBoost documentation, it can automatically handle categorical features and does not require one-hot encoding.\n",
    "Categorical values should be provided as strings or pandas 'object' dtype.\n",
    "Since CatBoost performs its own encoding, no manual encoding is applied in the Data Preparation phase.\n",
    "This activity checks that the primary categorical modeling features (Segment, Region, Category, Sub-Category) are correctly stored, ensuring CatBoost can process them without one-hot encoding.\n",
    "\"\"\"\n",
    "categorical_encoding_activity = [\n",
    "    ':verify_categorical_types rdf:type prov:Activity .',\n",
    "    ':verify_categorical_types sc:isPartOf :data_preparation_phase .',\n",
    "    ':verify_categorical_types rdfs:comment \"Data Preparation\" .',\n",
    "    f':verify_categorical_types rdfs:comment \"\"\"{ce_comment}\"\"\" .',\n",
    "    f':verify_categorical_types prov:startedAtTime \"{start_time_ce}\"^^xsd:dateTime .',\n",
    "    f':verify_categorical_types prov:endedAtTime \"{end_time_ce}\"^^xsd:dateTime .',\n",
    "    f':verify_categorical_types prov:qualifiedAssociation :{ce_ass_uuid_writer} .',\n",
    "    f':{ce_ass_uuid_writer} prov:agent :{categorical_encoding_code_writer} .',\n",
    "    f':{ce_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ce_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':verify_categorical_types prov:used :engineered_data .',\n",
    "    ':categorical_data rdf:type prov:Entity .',\n",
    "    ':categorical_data prov:wasGeneratedBy :verify_categorical_types .',\n",
    "    ':categorical_data prov:wasDerivedFrom :engineered_data .',\n",
    "]\n",
    "engine.insert(categorical_encoding_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6024f4-0d39-4769-865a-a44d51192add",
   "metadata": {},
   "source": [
    "**Plausibility and consistency checks were applied to validate the integrity of the prepared dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e67f86-c0c1-4d72-aa56-1d4d0ef9c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "plausibility_code_writer = student_b\n",
    "def check_plausibility_and_consistency(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Plausibility and consistency checks for the prepared data.\n",
    "    This function enforces the following rules:\n",
    "    1) Sales must be strictly positive.\n",
    "    2) Quantity must be strictly positive.\n",
    "    3) Discount values must lie within [0, 1].\n",
    "    4) Ship Date must be on or after Order Date.\n",
    "    5) If all rows belong to the United States, Postal Code must be a positive\n",
    "       4â5 digit value (integer ZIP code with possible loss of leading zeros).\n",
    "       \n",
    "    The function raises a ValueError if any violation is found and otherwise\n",
    "    returns the input DataFrame unchanged.\n",
    "    \"\"\"\n",
    "    df_check = df.copy()\n",
    "\n",
    "    # 1) Sales strictly positive\n",
    "    if (df_check[\"Sales\"] <= 0).any():\n",
    "        raise ValueError(\"Plausibility check failed: Sales must be strictly positive.\")\n",
    "\n",
    "    # 2) Quantity must be strictly positive\n",
    "    if (df_check[\"Quantity\"] <= 0).any():\n",
    "        raise ValueError(\"Plausibility check failed: Quantity must be strictly positive.\")\n",
    "\n",
    "    # 3) Discount in [0, 1]\n",
    "    invalid_discount_mask = (df_check[\"Discount\"] < 0) | (df_check[\"Discount\"] > 1)\n",
    "    if invalid_discount_mask.any():\n",
    "        raise ValueError(\"Plausibility check failed: Discount values must be in [0, 1].\")\n",
    "\n",
    "    # 4) Ship Date >= Order Date\n",
    "    invalid_date_mask = df_check[\"Ship Date\"] < df_check[\"Order Date\"]\n",
    "    if invalid_date_mask.any():\n",
    "        raise ValueError(\n",
    "            \"Consistency check failed: Ship Date must be greater than or equal to Order Date.\"\n",
    "        )\n",
    "\n",
    "    # 5) Postal Code plausibility (only if all rows are US)\n",
    "    countries = df_check[\"Country\"].unique()\n",
    "    if len(countries) == 1 and countries[0] == \"United States\":\n",
    "        postal_str = df_check[\"Postal Code\"].astype(str)\n",
    "        valid_length = postal_str.str.len().isin([4, 5])\n",
    "        valid_positive = df_check[\"Postal Code\"] > 0\n",
    "        valid_zip = valid_length & valid_positive\n",
    "\n",
    "        if (~valid_zip).any():\n",
    "            raise ValueError(\n",
    "                \"Consistency check failed: Postal Code values do not follow expected US ZIP rules.\"\n",
    "            )\n",
    "\n",
    "    return df_check\n",
    "\n",
    "start_time_pc = now()\n",
    "consistent_data = check_plausibility_and_consistency(categorical_data)\n",
    "end_time_pc = now()\n",
    "\n",
    "display(consistent_data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "pc_ass_uuid_executor = \"929e9112-3b38-49f3-a4ea-24448a21149e\"\n",
    "plausibility_check_executor = [\n",
    "    f':check_plausibility_and_consistency prov:qualifiedAssociation :{pc_ass_uuid_executor} .',\n",
    "    f':{pc_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{pc_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{pc_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(plausibility_check_executor, prefixes=prefixes)\n",
    "\n",
    "pc_ass_uuid_writer = \"0c32fee7-6295-431e-b790-6bba003f3c09\"\n",
    "\n",
    "pc_comment = \"\"\"\n",
    "Plausibility and consistency checks were applied to validate the integrity of the prepared dataset.\n",
    "These checks ensure that no logically invalid or inconsistent records remain in the dataset.\n",
    "The following rules were enforced:\n",
    "1) Sales must be strictly positive.\n",
    "2) Quantity must be strictly positive.\n",
    "3) Discount values must lie within [0, 1].\n",
    "4) Ship Date must be on or after Order Date.\n",
    "5) If all rows belong to the United States, Postal Code must be a positive 4â5 digit value (integer ZIP code with possible loss of leading zeros).\n",
    "\"\"\"\n",
    "\n",
    "plausibility_check_activity = [\n",
    "    ':check_plausibility_and_consistency rdf:type prov:Activity .',\n",
    "    ':check_plausibility_and_consistency sc:isPartOf :data_preparation_phase .',\n",
    "    ':check_plausibility_and_consistency rdfs:comment \"Data Preparation\" .',\n",
    "    f':check_plausibility_and_consistency rdfs:comment \"\"\"{pc_comment}\"\"\" .',\n",
    "    f':check_plausibility_and_consistency prov:startedAtTime \"{start_time_pc}\"^^xsd:dateTime .',\n",
    "    f':check_plausibility_and_consistency prov:endedAtTime \"{end_time_pc}\"^^xsd:dateTime .',\n",
    "    f':check_plausibility_and_consistency prov:qualifiedAssociation :{pc_ass_uuid_writer} .',\n",
    "    f':{pc_ass_uuid_writer} prov:agent :{plausibility_code_writer} .',\n",
    "    f':{pc_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{pc_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_plausibility_and_consistency prov:used :categorical_data .',\n",
    "    ':consistent_data rdf:type prov:Entity .',\n",
    "    ':consistent_data prov:wasGeneratedBy :check_plausibility_and_consistency .',\n",
    "    ':consistent_data prov:wasDerivedFrom :categorical_data .',\n",
    "]\n",
    "engine.insert(plausibility_check_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7a871-b598-41c2-865e-5755cc83a4a3",
   "metadata": {},
   "source": [
    "**Representation and bias analysis for key categorical attributes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bbef7-8c10-430b-bb3a-489fcaf80da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_code_writer = student_b\n",
    "def analyze_category_balance(df: pd.DataFrame, categorical_cols: list) -> dict:\n",
    "    \"\"\"\n",
    "    Summarize class balance for key categorical attributes.\n",
    "    For each column in categorical_cols this function computes:\n",
    "    - \"Segment\", \"Region\", \"Category\":\n",
    "        * absolute counts per attributes (global)\n",
    "        * relative frequency in percent over the full dataset\n",
    "    - \"Sub-Category\":\n",
    "        * counts of \"Sub-Category\" within each \"Category\"\n",
    "        * relative frequency in percent within each \"Category\"\n",
    "    Returns a nested dictionary with these summaries.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        # \"Segment\", \"Region\", \"Category\"\n",
    "        if col != \"Sub-Category\":\n",
    "            value_counts = df[col].value_counts()\n",
    "            rel_freq = value_counts / len(df) * 100.0\n",
    "\n",
    "            summary_df = pd.DataFrame({\n",
    "                \"count\": value_counts,\n",
    "                \"percentage\": rel_freq.round(2)\n",
    "            })\n",
    "\n",
    "            results[col] = summary_df\n",
    "            \n",
    "        # \"Sub-Category\"\n",
    "        else:\n",
    "            # counts: rows = Category, columns = Sub-Category\n",
    "            counts = (\n",
    "                df.groupby(\"Category\")[\"Sub-Category\"]\n",
    "                  .value_counts()\n",
    "                  .unstack(fill_value=0)\n",
    "            )\n",
    "\n",
    "            # within-category percentages\n",
    "            percentages = counts.div(counts.sum(axis=1), axis=0) * 100.0\n",
    "            percentages = percentages.round(2)\n",
    "\n",
    "            results[\"Sub-Category\"] = {\n",
    "                \"counts\": counts,\n",
    "                \"within_category_percentage\": percentages\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "# key categorical attributes\n",
    "key_categorical_attributes = [\n",
    "    \"Segment\",\n",
    "    \"Region\",\n",
    "    \"Category\",\n",
    "    \"Sub-Category\",\n",
    "]\n",
    "\n",
    "start_time_repr = now()\n",
    "representation_report = analyze_category_balance(consistent_data, key_categorical_attributes)\n",
    "end_time_repr = now()\n",
    "\n",
    "# build and print representation_summary:\n",
    "parts = []\n",
    "\n",
    "# \"Segment\", \"Region\", \"Category\"\n",
    "for col in [\"Segment\", \"Region\", \"Category\"]:\n",
    "    if col in representation_report:\n",
    "        df_col = representation_report[col]\n",
    "        parts.append(f\"= {col} distribution =\\n{df_col.to_string()}\\n\")\n",
    "        \n",
    "# \"Sub-Category\"\n",
    "counts = representation_report[\"Sub-Category\"][\"counts\"]\n",
    "pct = representation_report[\"Sub-Category\"][\"within_category_percentage\"]\n",
    "parts.append(f\"= Sub-Category counts by Category = \\n{counts.to_string()}\\n\")\n",
    "parts.append(\n",
    "    \"= Sub-Category within-category percentages =\\n\"\n",
    "    f\"{pct.to_string()}\\n\"\n",
    ")\n",
    "\n",
    "representation_summary = (\n",
    "    \"=== Representation Summary ===\\n\\n\"\n",
    "    + \"\\n\".join(parts)\n",
    ")\n",
    "\n",
    "print(representation_summary)\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "repr_ass_uuid_executor = \"a514c2bf-1a42-48e3-8838-63d679381d33\"\n",
    "representation_balance_executor = [\n",
    "    f':analyze_category_balance prov:qualifiedAssociation :{repr_ass_uuid_executor} .',\n",
    "    f':{repr_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{repr_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{repr_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(representation_balance_executor, prefixes=prefixes)\n",
    "\n",
    "repr_ass_uuid_writer = \"32f2aeb5-fe50-4517-80ee-b79aad9872c0\"\n",
    "repr_comment = \"\"\"\n",
    "Representation and bias analysis for key categorical attributes.\n",
    "We summarize the class balance of the main categorical variables:\n",
    "- \"Segment\", \"Region\", \"Category\":\n",
    "    * absolute counts per attributes (global)\n",
    "    * relative frequency in percent over the full dataset\n",
    "- \"Sub-Category\":\n",
    "    * counts of \"Sub-Category\" within each \"Category\"\n",
    "    * relative frequency in percent within each \"Category\"\n",
    "\n",
    "Findings:\n",
    "- Segment, Region and Category show slightly underrepresented classes.\n",
    "  But their groups still have reasonable frequencies relative to the full dataset,\n",
    "  so it is likely that no resampling or reweighting has to be applied at this stage.\n",
    "  Resampling could be reconsidered in case of poor performance of our model in the next phases.\n",
    "- For Sub-Category, the distribution is inspected within each Category.\n",
    "  Some Sub-Categories occur much less frequently than others inside the\n",
    "  same Category (some < 5% of entries in that Category) and are\n",
    "  therefore flagged as potentially underrepresented. Resampling or\n",
    "  reweighting might lead to better results in the Modeling phase.\n",
    "\"\"\"\n",
    "representation_balance_activity = [\n",
    "    ':analyze_category_balance rdf:type prov:Activity .',\n",
    "    ':analyze_category_balance sc:isPartOf :data_preparation_phase .',\n",
    "    ':analyze_category_balance rdfs:comment \"Data Preparation\" .',\n",
    "    f':analyze_category_balance rdfs:comment \"\"\"{repr_comment}\"\"\" .',\n",
    "    f':analyze_category_balance prov:startedAtTime \"{start_time_repr}\"^^xsd:dateTime .',\n",
    "    f':analyze_category_balance prov:endedAtTime \"{end_time_repr}\"^^xsd:dateTime .',\n",
    "    f':analyze_category_balance prov:qualifiedAssociation :{repr_ass_uuid_writer} .',\n",
    "    f':{repr_ass_uuid_writer} prov:agent :{representation_code_writer} .',\n",
    "    f':{repr_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{repr_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':analyze_category_balance prov:used :consistent_data .',\n",
    "    ':representation_report rdf:type prov:Entity .',\n",
    "    ':representation_report prov:wasGeneratedBy :analyze_category_balance .',\n",
    "    ':representation_report prov:wasDerivedFrom :consistent_data .',\n",
    "    f':representation_report rdfs:comment \"\"\"{representation_summary}\"\"\" .',\n",
    "]\n",
    "engine.insert(representation_balance_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36806d-9c14-4436-b275-1cda34ca151f",
   "metadata": {},
   "source": [
    "**Attribute Removal in the Data Preparation phase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8745a-06a4-48a2-bf08-4bc88be3d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_removal_executor = student_b\n",
    "def remove_attributes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove attributes that cause data leakage or provide no predictive value for Sales prediction:\n",
    "    - Profit (derived from Sales -> leakage)\n",
    "    - Ship Date (only known after sale -> leakage)\n",
    "    - Row ID, Order ID (pure identifiers)\n",
    "    - Country (only removed if the dataset contains exactly one unique value)\n",
    "    Returns: pd.DataFrame without the removed columns\n",
    "    \"\"\"\n",
    "    df_removed = df.copy()\n",
    "\n",
    "    cols_remove = [\"Profit\", \"Ship Date\", \"Row ID\", \"Order ID\"]\n",
    "    \n",
    "    if df_removed[\"Country\"].nunique() == 1:\n",
    "        cols_remove.append(\"Country\")\n",
    "\n",
    "    df_removed.drop(columns=cols_remove, errors=\"ignore\", inplace=True)\n",
    "\n",
    "    return df_removed\n",
    "\n",
    "start_time_attr_removal = now()\n",
    "removed_attributes_data = remove_attributes(consistent_data)\n",
    "end_time_attr_removal = now()\n",
    "\n",
    "display(removed_attributes_data)\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "ra_ass_uuid_executor = \"ea4cb861-531e-4f08-961f-bd23701b4abb\"\n",
    "remove_attributes_executor = [\n",
    "    f':remove_attributes prov:qualifiedAssociation :{ra_ass_uuid_executor} .',\n",
    "    f':{ra_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ra_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ra_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(remove_attributes_executor, prefixes=prefixes)\n",
    "\n",
    "ra_ass_uuid_writer = \"dded4f67-ed3e-428e-abbd-05e5f5a69897\"\n",
    "ra_comment = \"\"\"\n",
    "Remove attributes that cause data leakage or provide no predictive value for Sales prediction:\n",
    "- Profit (derived from Sales -> leakage)\n",
    "- Ship Date (only known after sale -> leakage)\n",
    "- Row ID, Order ID (pure identifiers)\n",
    "- Country (only removed if the dataset contains exactly one unique value)\n",
    "This step ensures a leakage-free and business-meaningful feature space for subsequent modeling.\n",
    "\"\"\"\n",
    "remove_attributes_activity = [\n",
    "    ':remove_attributes rdf:type prov:Activity .',\n",
    "    ':remove_attributes sc:isPartOf :data_preparation_phase .',\n",
    "    ':remove_attributes rdfs:comment \\'Data Preparation\\' .',\n",
    "    f':remove_attributes rdfs:comment \"\"\"{ra_comment}\"\"\" .',\n",
    "    f':remove_attributes prov:startedAtTime \"{start_time_attr_removal}\"^^xsd:dateTime .',\n",
    "    f':remove_attributes prov:endedAtTime \"{end_time_attr_removal}\"^^xsd:dateTime .',\n",
    "    f':remove_attributes prov:qualifiedAssociation :{ra_ass_uuid_writer} .',\n",
    "    f':{ra_ass_uuid_writer} prov:agent :{attr_removal_executor} .',\n",
    "    f':{ra_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ra_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':remove_attributes prov:used :consistent_data .',\n",
    "    ':removed_attributes_data rdf:type prov:Entity .',\n",
    "    ':removed_attributes_data prov:wasGeneratedBy :remove_attributes .',\n",
    "    ':removed_attributes_data prov:wasDerivedFrom :consistent_data .',\n",
    "]\n",
    "engine.insert(remove_attributes_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56150d6-1631-4f84-9986-6102f7000b2c",
   "metadata": {},
   "source": [
    "**Pre-processing steps considered but not applied**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392eb11f-77b9-4089-9160-106f47e05004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "dp_prepro_comment = \"\"\"\n",
    "Additional preprocessing steps were considered but not applied, as they did not\n",
    "provide benefits for our goals considering the fact that we plan to use CatBoost in the Modeling phase:\n",
    "- No scaling: CatBoost is tree-based and insensitive to feature scale.\n",
    "- No binning of numeric variables: binning has a risk of reducing information because CatBoost \n",
    "  handles continuous features natively anyways.\n",
    "- No additional encoding of categorical attributes: CatBoost supports raw \n",
    "  categorical variables without manual transformations.\n",
    "- No further handling of skewed numeric variables: although Sales was log-\n",
    "  transformed, remaining skew does not negatively impact CatBoost models.\n",
    "\n",
    "These steps were reviewed but intentionally left out to avoid unnecessary \n",
    "transformations and to preserve the interpretability of the original features.\n",
    "\"\"\"\n",
    "\n",
    "dp_prepro = [\n",
    "    ':data_considered_preprocessing rdf:type prov:Entity .',\n",
    "    ':data_considered_preprocessing sc:isPartOf :data_preparation_phase .',\n",
    "    f':data_considered_preprocessing rdfs:comment \"\"\"{dp_prepro_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(dp_prepro, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc978605-c523-4a5f-bdf8-041e955d2bdc",
   "metadata": {},
   "source": [
    "**Consideration of potential external data sources.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60df1b-5b80-43fa-9e4d-8274bbf801e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "dp_external_comment = \"\"\"\n",
    "Consideration of potential external data sources.\n",
    "\n",
    "During the Data Preparation phase we analyzed which additional external datasets could\n",
    "improve the predictive performance of our model. Examples of such sources include:\n",
    "\n",
    "- Macroeconomic indicators (inflation, consumer behaviour, GDP trends, etc.)\n",
    "    * e.g. Federal Reserve Economic Data FRED\n",
    "- Calendar-related data (public holidays, black friday, etc.)\n",
    "    * e.g. Worldwide public holidays 1970 - 2099 (Kaggle)\n",
    "- Competitor price or market trend\n",
    "- Marketing campaign schedules or promotion data\n",
    "\n",
    "The sources were only considered conceptually at this stage. They are not used in any \n",
    "way during the preparation of this dataset. But we see a potential that they could improve\n",
    "future versions of the model.\n",
    "\"\"\"\n",
    "\n",
    "dp_external = [\n",
    "    ':external_data_considered rdf:type prov:Entity .',\n",
    "    ':external_data_considered sc:isPartOf :data_preparation_phase .',\n",
    "    f':external_data_considered rdfs:comment \"\"\"{dp_external_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(dp_external, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19adaa76-a7f2-4058-aba9-50dab8147bef",
   "metadata": {},
   "source": [
    "**Final prepared dataset after the full Data Preparation phase and croissant documentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d7155-689e-445b-b43b-9882be3a6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = removed_attributes_data\n",
    "\n",
    "prepared_data_comment = \"\"\"\n",
    "Final prepared dataset after the full Data Preparation phase.\n",
    "\n",
    "All preprocessing decisions, transformations, and validation rules have been\n",
    "executed and documented in earlier steps. This dataset is now ready for the\n",
    "Modeling phase.\n",
    "\"\"\"\n",
    "\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    ':prepared_data sc:name \"Prepared Superstore Sales Dataset\" .',\n",
    "    f':prepared_data sc:description \"\"\"{prepared_data_comment}\"\"\" .',\n",
    "    ':prepared_data sc:isPartOf :data_preparation_phase .',\n",
    "    ':prepared_data prov:wasDerivedFrom :removed_attributes_data .',\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)\n",
    "\n",
    "prepared_recordset_triples = [\n",
    "    ':prepared_recordset rdf:type cr:RecordSet .',\n",
    "    ':prepared_recordset sc:name \"Prepared Superstore Table\" .',\n",
    "    ':prepared_recordset sc:description \"Cleaned and feature-enhanced tabular dataset used for model training.\" .',\n",
    "    ':prepared_data cr:recordSet :prepared_recordset .',\n",
    "\n",
    "    # fields still present after attribute removal:\n",
    "    ':prepared_recordset cr:field :field_order_date .',\n",
    "    ':prepared_recordset cr:field :field_ship_mode .',\n",
    "    ':prepared_recordset cr:field :field_customer_id .',\n",
    "    ':prepared_recordset cr:field :field_customer_name .',\n",
    "    ':prepared_recordset cr:field :field_segment .',\n",
    "    ':prepared_recordset cr:field :field_city .',\n",
    "    ':prepared_recordset cr:field :field_state .',\n",
    "    ':prepared_recordset cr:field :field_postal_code .',\n",
    "    ':prepared_recordset cr:field :field_region .',\n",
    "    ':prepared_recordset cr:field :field_product_id .',\n",
    "    ':prepared_recordset cr:field :field_category .',\n",
    "    ':prepared_recordset cr:field :field_sub_category .',\n",
    "    ':prepared_recordset cr:field :field_product_name .',\n",
    "    ':prepared_recordset cr:field :field_sales .',\n",
    "    ':prepared_recordset cr:field :field_quantity .',\n",
    "    ':prepared_recordset cr:field :field_discount .',\n",
    "\n",
    "    # derived fields added during preparation:\n",
    "    # Sales_log\n",
    "    ':prepared_recordset cr:field :field_sales_log .',\n",
    "    ':field_sales_log rdf:type cr:Field .',\n",
    "    ':field_sales_log sc:name \"Sales_log\" .',\n",
    "    ':field_sales_log sc:description \"Log-transformed Sales value to reduce right skew.\" .',\n",
    "    ':field_sales_log cr:dataType xsd:double .',\n",
    "\n",
    "    # order_year\n",
    "    ':prepared_recordset cr:field :field_order_year .',\n",
    "    ':field_order_year rdf:type cr:Field .',\n",
    "    ':field_order_year sc:name \"order_year\" .',\n",
    "    ':field_order_year sc:description \"Year extracted from Order Date.\" .',\n",
    "    ':field_order_year cr:dataType xsd:gYear .',\n",
    "\n",
    "    # order_month\n",
    "    ':prepared_recordset cr:field :field_order_month .',\n",
    "    ':field_order_month rdf:type cr:Field .',\n",
    "    ':field_order_month sc:name \"order_month\" .',\n",
    "    ':field_order_month sc:description \"Month extracted from Order Date (1â12).\" .',\n",
    "    ':field_order_month cr:dataType xsd:integer .',\n",
    "\n",
    "    # order_weekofyear\n",
    "    ':prepared_recordset cr:field :field_order_weekofyear .',\n",
    "    ':field_order_weekofyear rdf:type cr:Field .',\n",
    "    ':field_order_weekofyear sc:name \"order_weekofyear\" .',\n",
    "    ':field_order_weekofyear sc:description \"ISO week number extracted from Order Date.\" .',\n",
    "    ':field_order_weekofyear cr:dataType xsd:integer .',\n",
    "\n",
    "    # order_dayofweek\n",
    "    ':prepared_recordset cr:field :field_order_dayofweek .',\n",
    "    ':field_order_dayofweek rdf:type cr:Field .',\n",
    "    ':field_order_dayofweek sc:name \"order_dayofweek\" .',\n",
    "    ':field_order_dayofweek sc:description \"Day of week as integer (Monday=0, Sunday=6).\" .',\n",
    "    ':field_order_dayofweek cr:dataType xsd:integer .',\n",
    "\n",
    "    # order_dayofmonth\n",
    "    ':prepared_recordset cr:field :field_order_dayofmonth .',\n",
    "    ':field_order_dayofmonth rdf:type cr:Field .',\n",
    "    ':field_order_dayofmonth sc:name \"order_dayofmonth\" .',\n",
    "    ':field_order_dayofmonth sc:description \"Day of month extracted from Order Date (1â31).\" .',\n",
    "    ':field_order_dayofmonth cr:dataType xsd:integer .',\n",
    "]\n",
    "engine.insert(prepared_recordset_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"b3e840ab-ac23-415e-bd9c-6d00bb79c37a\"\n",
    "dma_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example algorithm definition\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # example implementation\n",
    "    f':random_forrest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forrest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "    # you can also define your Evaluation Measures here\n",
    "    \n",
    "    # example evaluation \n",
    "    f':r2_score_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':r2_score_measure rdfs:label \"R-squared Score\" .',\n",
    "    f':r2_score_measure rdfs:comment \"xxx\" .',\n",
    "    f':r2_score_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"fff582a8-c5cd-4030-978b-9f56b603167c\"\n",
    "hp_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example parameter\n",
    "    f':hp_learning_rate rdf:type mls:HyperParameter .',\n",
    "    f':hp_learning_rate rdfs:label \"Learning Rate\" .',\n",
    "    f':hp_learning_rate rdfs:comment \"...\" .',\n",
    "    f':random_forrest_classifier_implementation mls:hasHyperParameter :hp_learning_rate .',\n",
    "    f':hp_learning_rate prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # continue with your identified hyperparameters\n",
    "    \n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame):\n",
    "    #do something\n",
    "    return 'train_set', 'validation_set', 'test_set'\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"fb58ae6c-9d58-44c9-ac7e-529111bdf7fc\"\n",
    "split_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\" \n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "    \n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_model(training_set, validation_set):\n",
    "    # do something here\n",
    "\n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    return 'Find most suitable model'\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"21d60fe3-c9ab-4a0a-bae7-b9fe9653c755\"\n",
    "tafm_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\" \n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    ########################################\n",
    "    # ONE model run - automate everything below!\n",
    "\n",
    "    # Parameter settings\n",
    "    f':{hp1_setting_run1} rdf:type mls:HyperParameterSetting .',\n",
    "    f':{hp1_setting_run1} mls:specifiedBy :hp_learning_rate .',\n",
    "    f':{hp1_setting_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{hp1_setting_run1} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    # add your further parameters\n",
    "\n",
    "    # Describe your Run\n",
    "    f':{training_run1} rdf:type mls:Run .',\n",
    "    f':{training_run1} sc:isPartOf :train_and_finetune_model .',\n",
    "    f':{training_run1} mls:realizes :random_forest_algorithm .',\n",
    "    f':{training_run1} rdf:label \"Training Run 1 with...\" .',\n",
    "    f':{training_run1} mls:executes :your_implementation .', \n",
    "    f':{training_run1} mls:hasInput :training_set .',\n",
    "    f':{training_run1} mls:hasInput :validation_set .',\n",
    "    f':{training_run1} mls:hasInput :{hp1_setting_run1} .',     \n",
    "    # list all your used parameters here\n",
    "    f':{training_run1} mls:hasOutput :{model_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_train_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_validation_run1} .',\n",
    "\n",
    "    # Describe your Model\n",
    "    f':{model_run1} rdf:type mls:Model .',\n",
    "    f':{model_run1} prov:label \"xxx\" .',\n",
    "    f':{model_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{model_run1} mlso:trainedOn :training_set .',\n",
    "    f':{model_run1} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "\n",
    "    # Describe your evaluations\n",
    "    # You can have multiple evaluations per model \n",
    "    f':{eval_train_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_train_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_train_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_train_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_train_run1} prov:used :training_set .',\n",
    "\n",
    "    f':{eval_validation_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_validation_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_validation_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_validation_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_validation_run1} prov:used :validation_set .',\n",
    "\n",
    "    # Dont forget to document any visualizations\n",
    "\n",
    "]\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_full_data(training_set, validation_set):\n",
    "    \n",
    "    # create your\n",
    "    return \"Final Trained Model\"\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"96815ee0-524c-437b-b5fa-2e15b945c993\" # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model = \":final_model_entity\"\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_documentation = [\n",
    "    # your documentation here    \n",
    "]\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_code_writer = student_b\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "\n",
    "    # Predict and evaluation on test data\n",
    "        \n",
    "    return 'Performance'\n",
    "\n",
    "start_time_eval = now()\n",
    "#evaluate_on_test_data()\n",
    "end_time_eval = now() \n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"7f1431e9-feed-429a-92ed-c131b23cbe79\" # Generate once\n",
    "final_model = \":final_model_entity\" \n",
    "test_set = \":test_set\" \n",
    "\n",
    "eval_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "    \n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "    \n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    " \n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"...\" .',\n",
    "    \n",
    "]\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"72a921e0-1234-4567-89ab-cdef01234567\" # Generate once\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .', \n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty: # type:ignore\n",
    "    for _, row in res_authors.iterrows(): # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "        \n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?ds_comment ?bo_comment WHERE {{\n",
    "  OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds_comment . }}\n",
    "  OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo_comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {} # type:ignore\n",
    "bu_data_source = latex_escape(clean_rdf(row_bu.get(\"ds_comment\", \"\")))\n",
    "bu_objectives  = latex_escape(clean_rdf(row_bu.get(\"bo_comment\", \"\")))\n",
    "\n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :raw_data sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {} # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{\n",
    "  :raw_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  ?field sc:name ?name .\n",
    "  ?field sc:description ?descRaw .\n",
    "  ?field cr:dataType ?dtypeRaw .\n",
    "}} \n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "res_du = engine.query(du_query)\n",
    "du_rows = []\n",
    "if not res_du.empty: # type:ignore\n",
    "    for _, f in res_du.iterrows(): # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw: dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw: dtype = dtype_raw.split('/')[-1]\n",
    "        else: dtype = dtype_raw\n",
    "        \n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query)\n",
    "hp_rows = []\n",
    "if not res_hp.empty: #type:ignore\n",
    "    for _, row in res_hp.iterrows(): #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val  = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {} #type:ignore\n",
    "mod_algo  = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end   = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "Describe your Data preparation steps here and include respective graph data.\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
